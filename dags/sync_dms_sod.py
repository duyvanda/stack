# -*- coding: utf-8 -*-
"""sync_dms_sod.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HkCOrzeQ59g569nTKyt3uo1ORuIfy_K-
"""

from utils.df_handle import *

# from utils.df_handle import *
import pendulum
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator

local_tz = pendulum.timezone("Asia/Bangkok")

name='SOD'
prefix='SYNC_'
csv_path = '/usr/local/airflow/plugins'+'/'

dag_params = {
    'owner': 'airflow',
    "depends_on_past": False,
    'start_date': datetime(2022, 5, 14, tzinfo=local_tz),
    # 'email_on_failure': True,
    # 'email_on_retry': False,
    # 'email':['duyvq@merapgroup.com', 'vanquangduy10@gmail.com'],
    'do_xcom_push': False,
    'execution_timeout':timedelta(seconds=300)
    # 'retries': 3,
    # 'retry_delay': timedelta(minutes=10),
}

dag = DAG(prefix+name,
          catchup=False,
          default_args=dag_params,
          schedule_interval= '*/30 8-20,23-23 * * *',
          tags=[prefix+name, 'Sync', '30mins']
)

datenow = datetime.now().strftime("%Y-%m-%d")
datenow_mns1 = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
fdom = datetime.now().replace(day=1).strftime("%Y%m%d")

from_tb = "OM_SalesOrdDet"

usql = \
f"""
DECLARE @from DATE = '{datenow}'
DECLARE @to DATE = '2022-01-31'
SELECT
CONCAT(BranchID, OrderNbr, LineRef) as pk,
BranchID,
OrderNbr,
LineRef,
FreeItem,
InvtID,
LineQty,
OrderType,
OrigOrderNbr,
SiteID,
Crtd_Prog,
Crtd_User,
Crtd_Datetime,
LUpd_Datetime,
SlsperID,
OriginalLineRef,
BeforeVATPrice,
BeforeVATAmount,
AfterVATPrice,
AfterVATAmount,
VATAmount,
DiscAmt,
DocDiscAmt,
GroupDiscAmt1
from {from_tb}
where cast(LUpd_DateTime as DATE) >= @from
"""

# print(usql)

# print(sql)

table_name = "sync_dms_sod1"
table_temp = "sync_dms_sod_temp"



def insert():
    print(usql)

def update():
    try:
        #UPDATE
        df_update = get_ms_df(usql)
        assert df_update.shape[0] >0,"NO DATA TO INPUT"
        df_update.columns = lower_col(df_update)
        print (df_update.columns)
        df_update['crtd_datetime1'] = df_update['crtd_datetime'].dt.normalize()
        df_update1 = df_update['crtd_datetime1']
        drop_cols(df_update, "crtd_datetime1")
        df_update1.drop_duplicates(inplace=True)
        tpl_dt = tuple(df_update1.dt.strftime('%Y-%m-%d').to_list()) + ('1900-01-01','1900-01-01')
        # tpl_dt
        df_update1 = df_update['pk']
        df_update1.drop_duplicates(inplace=True)
        tpl_pk = tuple(df_update1.to_list()) + ('','')
        del_sql = \
        f"""
        DELETE FROM biteam.{table_name}
        WHERE
        DATE(crtd_datetime) in {tpl_dt}
        AND pk in {tpl_pk}
        """
        print("del_sql ",del_sql)
        execute_bq_query(del_sql)
        # DELTE CAC DON TAO VA XOA TRONG NGAY, DMS KHONG CO STATUS DELETED
        dsql = \
        f"""
        delete from biteam.{table_name} where date(crtd_datetime) >= '{datenow}'
        """
        print("delete_sql: ", dsql)
        execute_bq_query(dsql)
        #
        df_update['inserted_at'] = datetime.now()
        bq_values_insert(df_update, f"{table_name}", 2)
        # execute_bq_query(del_sql)
    except AssertionError:
        print("NO DATA TO INPUT")

dummy_start = DummyOperator(task_id="dummy_start", dag=dag)

insert = PythonOperator(task_id="insert", python_callable=insert, dag=dag)

update = PythonOperator(task_id="update", python_callable=update, dag=dag)

dummy_start >> insert >> update

